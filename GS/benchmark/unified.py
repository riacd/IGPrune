"""
ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ¡†æ¶

ä½¿ç”¨æ¨¡å‹æ³¨å†Œæœºåˆ¶ï¼Œæ”¯æŒå¼€å‘æ¨¡å‹å’Œbaselineæ¨¡å‹çš„ç»Ÿä¸€æµ‹è¯•å’Œæ¯”è¾ƒã€‚
"""

import torch
from torch_geometric.data import Data
from typing import Dict, List, Tuple, Optional, Any, Union
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import time
import json
import numpy as np
import networkx as nx
from torch_geometric.utils import to_networkx
import gc
import psutil
import os

from ..models import model_registry, GraphSummarizationModel, DownstreamModel
from ..datasets import DatasetLoader
from ..metrics import ComplexityMetric, InformationMetric, AccuracyMetric, SNRAnalysis, ICAnalysis


class UnifiedBenchmark:
    """
    ç»Ÿä¸€åŸºå‡†æµ‹è¯•ç±»

    æ”¯æŒé€šè¿‡æ¨¡å‹åç§°è‡ªåŠ¨åˆ›å»ºå’Œæµ‹è¯•ä»»æ„Graph Summarizationæ¨¡å‹ã€‚
    åŒ…å«å†…å­˜ä¼˜åŒ–å’ŒCUDAå†…å­˜ç®¡ç†åŠŸèƒ½ã€‚
    """

    def __init__(self,
                 results_dir: str = './results/unified_benchmark',
                 device: str = 'cuda',
                 data_dir: str = './data',
                 random_seed: int = 42,
                 memory_monitor: bool = True):
        """
        åˆå§‹åŒ–ç»Ÿä¸€åŸºå‡†æµ‹è¯•

        Args:
            results_dir: ç»“æœä¿å­˜ç›®å½•
            device: è®¡ç®—è®¾å¤‡
            data_dir: æ•°æ®ç›®å½•
            random_seed: éšæœºç§å­ï¼Œç¡®ä¿ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´
            memory_monitor: æ˜¯å¦å¯ç”¨å†…å­˜ç›‘æ§
        """
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.data_dir = data_dir  # ä¿å­˜data_dirä»¥ä¾¿åç»­ä½¿ç”¨
        self.dataset_loader = DatasetLoader(data_dir)
        self.random_seed = random_seed
        self.memory_monitor = memory_monitor

        # åˆå§‹åŒ–åº¦é‡
        self.complexity_metric = ComplexityMetric()

        # å®éªŒè®¾ç½®è¿½è¸ª
        self.experiment_configs = {}  # å­˜å‚¨å®éªŒé…ç½®

        # Memory management settings
        self.enable_memory_optimization = True
        if torch.cuda.is_available():
            # Set CUDA memory allocation configuration
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

    def _get_experiment_id(self, dataset_name: str, task_type: str, downstream_model: str) -> str:
        """ç”Ÿæˆå®éªŒè®¾ç½®ID"""
        return f"{dataset_name}_{task_type}_{downstream_model}"

    def _create_experiment_structure(self, dataset_name: str, task_type: str, downstream_model: str) -> Path:
        """åˆ›å»ºå®éªŒç»“æœçš„ç›®å½•ç»“æ„"""
        exp_id = self._get_experiment_id(dataset_name, task_type, downstream_model)

        # åˆ›å»ºå®éªŒç›®å½•ç»“æ„
        exp_dir = self.results_dir / exp_id
        exp_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        (exp_dir / "process_results").mkdir(exist_ok=True)
        (exp_dir / "comprehensive_results").mkdir(exist_ok=True)
        (exp_dir / "summary_graphs").mkdir(exist_ok=True)
        (exp_dir / "graph_visualizations").mkdir(exist_ok=True)
        (exp_dir / "training_curves").mkdir(exist_ok=True)

        # è®°å½•å®éªŒé…ç½®
        self.experiment_configs[exp_id] = {
            'dataset': dataset_name,
            'task_type': task_type,
            'downstream_model': downstream_model,
            'experiment_dir': exp_dir,
            'timestamp': time.strftime('%Y-%m-%d_%H-%M-%S')
        }

        return exp_dir
        
    def run_single_model(self,
                        model_name: str,
                        dataset_name: str = 'Cora',
                        task_type: str = 'original',
                        downstream_model: str = 'gcn',
                        num_steps: int = 10,
                        epochs: int = 100,
                        model_kwargs: Dict = None,
                        downstream_kwargs: Dict = None) -> Dict[str, Any]:
        """
        æµ‹è¯•å•ä¸ªæ¨¡å‹çš„æ€§èƒ½

        Args:
            model_name: æ³¨å†Œçš„æ¨¡å‹åç§°
            dataset_name: æ•°æ®é›†åç§°
            task_type: æ ‡ç­¾ä»»åŠ¡ç±»å‹ ('original' æˆ– 'degree')
            downstream_model: ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹ç±»å‹
            num_steps: å›¾æ€»ç»“æ­¥æ•°
            epochs: ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒè½®æ•°
            model_kwargs: æ¨¡å‹åˆå§‹åŒ–å‚æ•°
            downstream_kwargs: ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹å‚æ•°

        Returns:
            åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•æ¨¡å‹: {model_name} on {dataset_name} ({task_type} task)")
        print(f"{'='*80}")

        # åˆ›å»ºå®éªŒç›®å½•ç»“æ„
        exp_dir = self._create_experiment_structure(dataset_name, task_type, downstream_model)
        exp_id = self._get_experiment_id(dataset_name, task_type, downstream_model)

        # å‚æ•°é»˜è®¤å€¼
        model_kwargs = model_kwargs or {}
        downstream_kwargs = downstream_kwargs or {}
        
        # åŠ è½½æ•°æ®é›†
        print(f"åŠ è½½æ•°æ®é›† {dataset_name} with {task_type} task...")

        # Handle PPI multi-label tasks
        if dataset_name == 'PPI' and task_type == 'original':
            # Extract PPI label index from kwargs if provided
            ppi_label_index = model_kwargs.pop('ppi_label_index', 0)  # Remove from model_kwargs
            original_graph, train_mask, val_mask, test_mask = self.dataset_loader.load_dataset(
                dataset_name, task_type=task_type, ppi_label_index=ppi_label_index)
        else:
            original_graph, train_mask, val_mask, test_mask = self.dataset_loader.load_dataset(
                dataset_name, task_type=task_type)

        original_graph = self.dataset_loader.preprocess_for_summarization(original_graph)
        
        input_dim = original_graph.x.size(1)
        print(f"å›¾: {original_graph.num_nodes}èŠ‚ç‚¹, {original_graph.edge_index.shape[1]}è¾¹")
        print(f"ç‰¹å¾ç»´åº¦: {input_dim}")
        
        # åˆ›å»ºå›¾æ€»ç»“æ¨¡å‹
        print(f"åˆ›å»ºå›¾æ€»ç»“æ¨¡å‹: {model_name}")
        try:
            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = model_registry.get_model_info(model_name)
            print(f"æ¨¡å‹ç±»åˆ«: {model_info['category']}")
            print(f"æè¿°: {model_info['description']}")
            
            # è‡ªåŠ¨è®¾ç½®éœ€è¦input_dimçš„æ¨¡å‹å‚æ•°ï¼ˆæ‰€æœ‰å¼€å‘æ¨¡å‹éƒ½éœ€è¦ï¼‰
            if model_info['category'] == 'development':
                if 'input_dim' not in model_kwargs:
                    model_kwargs['input_dim'] = input_dim
                    print(f"  è‡ªåŠ¨è®¾ç½®input_dim = {input_dim}")
                if 'device' not in model_kwargs:
                    model_kwargs['device'] = self.device
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            gs_model = model_registry.create_model(model_name, **model_kwargs)
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            return {'error': str(e)}
        
        # åˆ›å»ºä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹
        print(f"åˆ›å»ºä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹: {downstream_model}")
        if downstream_model.lower() == 'gcn':
            from ..models import GCNDownstreamModel
            dt_model = GCNDownstreamModel(
                input_dim=input_dim, 
                device=self.device,
                **downstream_kwargs
            )
        elif downstream_model.lower() == 'gat':
            from ..models import GATDownstreamModel
            dt_model = GATDownstreamModel(
                input_dim=input_dim,
                device=self.device, 
                **downstream_kwargs
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹: {downstream_model}")
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœæ˜¯å¼€å‘æ¨¡å‹ï¼‰
        training_time = 0
        training_history = None
        if model_info['category'] == 'development':
            print(f"ğŸš€ è®­ç»ƒ{model_name}æ¨¡å‹ ...")
            print(f"ğŸ“‹ è®­ç»ƒå’Œæµ‹è¯•å°†ä½¿ç”¨ç›¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹: {downstream_model}")
            start_time = time.time()
            
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯Neural-Enhancedæ¨¡å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                if 'neural_enhanced' in model_name:
                    # Neural-Enhancedæ¨¡å‹æœ‰è‡ªå·±çš„åŒ…è£…å™¨
                    from ..models.neural_enhanced_gradient import TrainableNeuralEnhancedGradientModel

                    # è®¾ç½®è®­ç»ƒæ•°æ®
                    gs_model.set_training_data(train_mask, val_mask, original_graph.y)

                    # åˆ›å»ºå¯è®­ç»ƒåŒ…è£…å™¨
                    trainable_model = TrainableNeuralEnhancedGradientModel(
                        model=gs_model,
                        training_strategy='fixed_uniform'
                    )

                    # è®­ç»ƒæ¨¡å‹
                    training_history = trainable_model.train_model(
                        graph=original_graph,
                        train_mask=train_mask,
                        val_mask=val_mask,
                        labels=original_graph.y,
                        epochs=epochs,  # ä½¿ç”¨ä¼ å…¥çš„epochså‚æ•°
                        num_steps=num_steps,
                        downstream_epochs=20  # ä¸‹æ¸¸æ¨¡å‹è®­ç»ƒè½®æ•°
                    )

                    # ä½¿ç”¨åŒ…è£…å™¨çš„æ¨¡å‹
                    gs_model = trainable_model.model

                else:
                    # æ™®é€šå¼€å‘æ¨¡å‹ä½¿ç”¨åŸæœ‰çš„è®­ç»ƒé€»è¾‘
                    from ..models.training_variants import TrainableGraphSummarizationModel

                    # åˆ›å»ºä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹å·¥å‚å‡½æ•°ï¼Œç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹
                    def downstream_model_factory():
                        # å¤åˆ¶å½“å‰çš„ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹é…ç½®
                        if downstream_model.lower() == 'gcn':
                            from ..models import GCNDownstreamModel
                            return GCNDownstreamModel(
                                input_dim=input_dim,
                                device=self.device,
                                **downstream_kwargs
                            )
                        elif downstream_model.lower() == 'gat':
                            from ..models import GATDownstreamModel
                            return GATDownstreamModel(
                                input_dim=input_dim,
                                device=self.device,
                                **downstream_kwargs
                            )
                        else:
                            raise ValueError(f"ä¸æ”¯æŒçš„ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹: {downstream_model}")

                    # åŒ…è£…æ¨¡å‹ä½¿å…¶å¯è®­ç»ƒï¼Œä¼ å…¥ç›¸åŒçš„ä¸‹æ¸¸æ¨¡å‹å·¥å‚
                    trainable_model = TrainableGraphSummarizationModel(
                        model=gs_model,
                        downstream_model_factory=downstream_model_factory
                    )

                    # è®­ç»ƒæ¨¡å‹
                    training_history = trainable_model.train(
                        graph=original_graph,
                        train_labels=original_graph.y,
                        train_mask=train_mask,
                        val_mask=val_mask,
                        num_epochs=50,  # Reduced for benchmark efficiency
                        num_steps=num_steps
                    )

                    # æ›´æ–°gs_modelä¸ºè®­ç»ƒåçš„æ¨¡å‹
                    gs_model = trainable_model.model
                
                training_time = time.time() - start_time
                print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}s")
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                return {'error': f'Training failed: {e}'}
        
        # ç”Ÿæˆæ€»ç»“å›¾
        print(f"ğŸ“Š ç”Ÿæˆ{num_steps+1}ä¸ªæ€»ç»“å›¾...")
        self._log_memory_usage("Before graph summarization")

        start_time = time.time()
        try:
            # Clear CUDA cache before summarization
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            summary_graphs = gs_model.summarize(original_graph, num_steps=num_steps)
            summarization_time = time.time() - start_time
            print(f"âœ… æ€»ç»“ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {summarization_time:.2f}s")

            self._log_memory_usage("After graph summarization")

        except Exception as e:
            if "out of memory" in str(e).lower():
                print(f"âŒ å›¾æ€»ç»“ç”Ÿæˆå¤±è´¥: CUDAå†…å­˜ä¸è¶³")
                print(f"å°è¯•å‡å°‘æ­¥æ•°æˆ–ä½¿ç”¨CPU...")
                # Try with CPU if CUDA OOM
                if self.device.type == 'cuda':
                    cpu_device = torch.device('cpu')
                    original_graph = original_graph.to(cpu_device)
                    gs_model.device = cpu_device
                    if hasattr(gs_model, 'model') and gs_model.model is not None:
                        gs_model.model = gs_model.model.to(cpu_device)
                    summary_graphs = gs_model.summarize(original_graph, num_steps=num_steps)
                    summarization_time = time.time() - start_time
                    print(f"âœ… CPUæ€»ç»“ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {summarization_time:.2f}s")
                else:
                    raise e
            else:
                print(f"âŒ å›¾æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
                return {'error': f'Summarization failed: {e}'}
        
        # æ‰“å°è¾¹æ•°ç»Ÿè®¡
        print("æ€»ç»“å›¾è¾¹æ•°:")
        for i, graph in enumerate(summary_graphs):
            print(f"  æ­¥éª¤ {i}: {graph.edge_index.shape[1]} è¾¹")
        
        # è®¡ç®—åº¦é‡æŒ‡æ ‡
        print("ğŸ“ˆ è®¡ç®—åº¦é‡æŒ‡æ ‡...")
        complexity_metrics = self.complexity_metric.compute_list(summary_graphs, original_graph)

        self._log_memory_usage("Before information metric computation")

        # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­åˆå§‹åŒ–InformationMetricï¼Œç¡®ä¿ä¸‹æ¸¸æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´
        info_metric = InformationMetric(dt_model, self.device, random_seed=self.random_seed)
        print(f"ğŸ§  è®­ç»ƒ{len(summary_graphs)}ä¸ªä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹è¿›è¡Œä¿¡æ¯åº¦é‡...")

        # Reduce epochs for memory efficiency if needed
        adaptive_epochs = self._get_adaptive_epochs(epochs, len(summary_graphs))
        if adaptive_epochs != epochs:
            print(f"âš¡ ä¸ºèŠ‚çº¦å†…å­˜ï¼Œå‡å°‘è®­ç»ƒè½®æ•°: {epochs} -> {adaptive_epochs}")

        # è®¡ç®—ä¸¤ç§å½’ä¸€åŒ–çš„ä¿¡æ¯åº¦é‡
        info_metrics_log = None
        info_metrics_add = None

        try:
            print("ğŸ“Š è®¡ç®—å¯¹æ•°æ¯”ç‡å½’ä¸€åŒ–ä¿¡æ¯åº¦é‡...")
            info_metrics_log = info_metric.compute_list(
                summary_graphs, train_mask, val_mask, test_mask,
                original_graph.y, epochs=adaptive_epochs, normalization='log_ratio'
            )

            print("ğŸ“Š è®¡ç®—åŠ æ³•å½’ä¸€åŒ–ä¿¡æ¯åº¦é‡...")
            # Reset model for different normalization calculation
            info_metric_add = InformationMetric(dt_model, self.device, random_seed=self.random_seed)
            info_metrics_add = info_metric_add.compute_list(
                summary_graphs, train_mask, val_mask, test_mask,
                original_graph.y, epochs=adaptive_epochs, normalization='additive'
            )

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"âŒ ä¿¡æ¯åº¦é‡è®¡ç®—å¤±è´¥: CUDAå†…å­˜ä¸è¶³")
                print(f"å°è¯•è¿›ä¸€æ­¥å‡å°‘è®­ç»ƒè½®æ•°...")
                # Try with even fewer epochs
                emergency_epochs = max(adaptive_epochs // 2, 10)
                print(f"ğŸš¨ ç´§æ€¥æ¨¡å¼: ä½¿ç”¨ {emergency_epochs} è½®è®­ç»ƒ")

                info_metrics_log = info_metric.compute_list(
                    summary_graphs, train_mask, val_mask, test_mask,
                    original_graph.y, epochs=emergency_epochs, normalization='log_ratio'
                )

                info_metric_add = InformationMetric(dt_model, self.device, random_seed=self.random_seed)
                info_metrics_add = info_metric_add.compute_list(
                    summary_graphs, train_mask, val_mask, test_mask,
                    original_graph.y, epochs=emergency_epochs, normalization='additive'
                )
            else:
                raise e

        self._log_memory_usage("After information metric computation")

        # è®¡ç®—Accuracyåº¦é‡
        print("ğŸ¯ è®¡ç®—Accuracyåº¦é‡...")
        accuracy_metrics = None
        try:
            accuracy_metric = AccuracyMetric(dt_model, self.device, random_seed=self.random_seed)
            accuracy_metrics = accuracy_metric.compute_list(
                summary_graphs, train_mask, val_mask, test_mask,
                original_graph.y, epochs=adaptive_epochs
            )
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"âŒ Accuracyåº¦é‡è®¡ç®—å¤±è´¥: CUDAå†…å­˜ä¸è¶³")
                print(f"å°è¯•è¿›ä¸€æ­¥å‡å°‘è®­ç»ƒè½®æ•°...")
                emergency_epochs = max(adaptive_epochs // 2, 10)
                print(f"ğŸš¨ ç´§æ€¥æ¨¡å¼: ä½¿ç”¨ {emergency_epochs} è½®è®­ç»ƒ")

                accuracy_metric = AccuracyMetric(dt_model, self.device, random_seed=self.random_seed)
                accuracy_metrics = accuracy_metric.compute_list(
                    summary_graphs, train_mask, val_mask, test_mask,
                    original_graph.y, epochs=emergency_epochs
                )
            else:
                raise e

        self._log_memory_usage("After accuracy metric computation")

        # è®¡ç®—IC-AUCå’Œä¿¡æ¯é˜ˆå€¼ç‚¹ (ä½¿ç”¨ä¸¤ç§å½’ä¸€åŒ–)
        from ..metrics import ICAnalysis

        ic_auc_log = ICAnalysis.compute_ic_auc(complexity_metrics, info_metrics_log)
        ic_auc_add = ICAnalysis.compute_ic_auc(complexity_metrics, info_metrics_add)

        threshold_point_log = ICAnalysis.compute_information_threshold_point(
            complexity_metrics, info_metrics_log, threshold=0.8)
        threshold_point_add = ICAnalysis.compute_information_threshold_point(
            complexity_metrics, info_metrics_add, threshold=0.8)

        # ä¿æŒå‘åå…¼å®¹
        snr_auc = ic_auc_add  # é»˜è®¤ä½¿ç”¨åŠ æ³•å½’ä¸€åŒ–
        
        # æ•´ç†ç»“æœ
        result = {
            'model_name': model_name,
            'model_info': model_info,
            'dataset': dataset_name,
            'task_type': task_type,
            'downstream_model': downstream_model,
            'num_steps': num_steps,
            'epochs': epochs,
            'complexity_metrics': complexity_metrics,
            # åŒé‡å½’ä¸€åŒ–ä¿¡æ¯åº¦é‡
            'information_metrics_log_ratio': info_metrics_log,
            'information_metrics_additive': info_metrics_add,
            # Accuracyåº¦é‡
            'accuracy_metrics': accuracy_metrics,
            # IC-AUCæŒ‡æ ‡ï¼ˆä¸¤ç§å½’ä¸€åŒ–ï¼‰
            'ic_auc_log_ratio': ic_auc_log,
            'ic_auc_additive': ic_auc_add,
            # ä¿¡æ¯é˜ˆå€¼ç‚¹ï¼ˆä¸¤ç§å½’ä¸€åŒ–ï¼‰
            'threshold_point_log_ratio': threshold_point_log,
            'threshold_point_additive': threshold_point_add,
            # å‘åå…¼å®¹çš„å­—æ®µ
            'information_metrics': info_metrics_add,  # é»˜è®¤ä½¿ç”¨åŠ æ³•å½’ä¸€åŒ–
            'snr_auc': snr_auc,  # å‘åå…¼å®¹
            'training_time': training_time,
            'summarization_time': summarization_time,
            'training_history': training_history,
            'summary_graphs': summary_graphs,
            'original_graph': original_graph,
            'exp_dir': exp_dir,
            'exp_id': exp_id,
            'success': True
        }

        # ä¿å­˜è¿‡ç¨‹ç»“æœ
        self._save_process_results(result)

        # æ‰“å°ç»“æœæ‘˜è¦
        print(f"\n{'='*80}")
        print("æµ‹è¯•ç»“æœ")
        print(f"{'='*80}")
        print(f"æ¨¡å‹: {model_name} ({model_info['category']})")
        print(f"SNR-AUC: {snr_auc:.4f}")
        print(f"å¤æ‚åº¦æŒ‡æ ‡: {[f'{x:.0f}' for x in complexity_metrics]}")
        print(f"ä¿¡æ¯æŒ‡æ ‡(log): {[f'{x:.4f}' for x in info_metrics_log]}")
        print(f"ä¿¡æ¯æŒ‡æ ‡(add): {[f'{x:.4f}' for x in info_metrics_add]}")
        if accuracy_metrics is not None:
            print(f"å‡†ç¡®åº¦æŒ‡æ ‡: {[f'{x:.4f}' for x in accuracy_metrics]}")

        # ç”Ÿæˆå•ä¸ªæ¨¡å‹çš„SNRæ›²çº¿å›¾ï¼ˆä¿å­˜åˆ°å®éªŒç›®å½•ï¼‰
        self._plot_single_model_enhanced(result)
        
        # Final memory cleanup
        if self.enable_memory_optimization:
            self._cleanup_memory()

        return result
    
    def compare_models(self,
                      model_names: List[str],
                      dataset_name: str = 'Cora',
                      task_type: str = 'original',
                      downstream_model: str = 'gcn',
                      num_steps: int = 10,
                      epochs: int = 100) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½

        Args:
            model_names: æ¨¡å‹åç§°åˆ—è¡¨
            dataset_name: æ•°æ®é›†åç§°
            task_type: æ ‡ç­¾ä»»åŠ¡ç±»å‹ ('original' æˆ– 'degree')
            downstream_model: ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹
            num_steps: å›¾æ€»ç»“æ­¥æ•°
            epochs: è®­ç»ƒè½®æ•°

        Returns:
            åŒ…å«æ‰€æœ‰æ¨¡å‹ç»“æœçš„å­—å…¸
        """
        print(f"\n{'='*100}")
        print(f"æ¨¡å‹æ¯”è¾ƒ: {len(model_names)} ä¸ªæ¨¡å‹ on {dataset_name} ({task_type} task)")
        print(f"{'='*100}")
        print(f"æ¨¡å‹åˆ—è¡¨: {', '.join(model_names)}")
        
        all_results = {}
        successful_results = []
        
        for model_name in model_names:
            try:
                # å¯¹äºéœ€è¦ç‰¹æ®Šå‚æ•°çš„æ¨¡å‹ï¼Œè‡ªåŠ¨é…ç½®
                model_kwargs = {}
                
                result = self.run_single_model(
                    model_name=model_name,
                    dataset_name=dataset_name,
                    task_type=task_type,
                    downstream_model=downstream_model,
                    num_steps=num_steps,
                    epochs=epochs,
                    model_kwargs=model_kwargs
                )
                
                all_results[model_name] = result
                if result.get('success', False):
                    successful_results.append(result)
                    
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")
                all_results[model_name] = {'error': str(e), 'success': False}
        
        if not successful_results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
            return all_results
        
        # ç”Ÿæˆç»¼åˆç»“æœ
        if successful_results:
            # ä»ç¬¬ä¸€ä¸ªæˆåŠŸç»“æœè·å–å®éªŒé…ç½®
            first_result = successful_results[0]
            exp_id = first_result['exp_id']
            exp_dir = first_result['exp_dir']

            # ä¿å­˜ç»¼åˆç»“æœ
            self._save_comprehensive_results(successful_results, exp_dir)

        # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆå…¼å®¹åŸæœ‰åŠŸèƒ½ï¼‰
        self._save_detailed_results(successful_results, dataset_name, downstream_model)

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_comparison_report(successful_results, dataset_name, downstream_model)

        # ç»˜åˆ¶å¯¹æ¯”å›¾
        self._plot_comparison(successful_results, dataset_name, downstream_model)
        
        return all_results

    def _save_process_results(self, result: Dict):
        """ä¿å­˜å•ä¸ªæ¨¡å‹çš„è¿‡ç¨‹ç»“æœ"""
        exp_dir = result['exp_dir']
        model_name = result['model_name']

        # 1. ä¿å­˜ç®€åŒ–è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„åº¦é‡æ•°æ® (TSVæ ¼å¼)
        self._save_step_metrics(result)

        # 2. ä¿å­˜å¤šæ­¥ç®€åŒ–å›¾ (CSVæ ¼å¼ + å¯è§†åŒ–)
        self._save_summary_graphs(result)

        # 3. ä¿å­˜è®­ç»ƒæ›²çº¿ (å¦‚æœæœ‰è®­ç»ƒè¿‡ç¨‹)
        if result['training_history'] is not None:
            self._save_training_curves(result)

        print(f"âœ… è¿‡ç¨‹ç»“æœå·²ä¿å­˜åˆ°: {exp_dir}")

    def _save_step_metrics(self, result: Dict):
        """ä¿å­˜æ¯æ­¥çš„å¤æ‚åº¦å’Œä¿¡æ¯åº¦é‡æ•°æ®ï¼ˆæ”¯æŒåŒé‡å½’ä¸€åŒ–ï¼‰"""
        exp_dir = result['exp_dir']
        model_name = result['model_name']

        # æ£€æŸ¥å¿…è¦çš„å­—æ®µæ˜¯å¦å­˜åœ¨
        complexity_metrics = result.get('complexity_metrics', [])
        info_metrics_log = result.get('information_metrics_log_ratio', [])
        info_metrics_add = result.get('information_metrics_additive', [])
        accuracy_metrics = result.get('accuracy_metrics', [])

        # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰åŒé‡å½’ä¸€åŒ–æ•°æ®ï¼Œä½¿ç”¨æ—§å­—æ®µ
        if not info_metrics_log and not info_metrics_add:
            info_metrics_old = result.get('information_metrics', [])
            if info_metrics_old:
                print(f"âš ï¸ {model_name}: ä½¿ç”¨å‘åå…¼å®¹æ¨¡å¼ï¼Œå°†å•ä¸€ä¿¡æ¯åº¦é‡å¤åˆ¶ä¸ºåŒé‡æ•°æ®")
                info_metrics_log = info_metrics_old
                info_metrics_add = info_metrics_old

        if not complexity_metrics or not info_metrics_log or not info_metrics_add:
            print(f"âŒ {model_name}: ç¼ºå°‘å¿…è¦çš„åº¦é‡æ•°æ®")
            print(f"   complexity_metrics: {len(complexity_metrics)} é¡¹")
            print(f"   info_metrics_log_ratio: {len(info_metrics_log)} é¡¹")
            print(f"   info_metrics_additive: {len(info_metrics_add)} é¡¹")
            print(f"   accuracy_metrics: {len(accuracy_metrics)} é¡¹")
            return

        # åˆ›å»ºæ­¥éª¤åº¦é‡æ•°æ®
        step_data = []

        # å‡†å¤‡æ•°æ®è¿›è¡Œzipæ“ä½œ
        zip_data = [complexity_metrics, info_metrics_log, info_metrics_add]
        zip_names = ['complexity', 'info_log', 'info_add']

        # å¦‚æœæœ‰accuracyæ•°æ®ï¼Œæ·»åŠ åˆ°zipä¸­
        if accuracy_metrics and len(accuracy_metrics) == len(complexity_metrics):
            zip_data.append(accuracy_metrics)
            zip_names.append('accuracy')

        for i, values in enumerate(zip(*zip_data)):
            step_dict = {
                'step': i,
                'complexity_metric': values[0],  # complexity
                'information_metric_log_ratio': values[1],  # info_log
                'information_metric_additive': values[2],  # info_add
                'model': model_name,
                'dataset': result['dataset'],
                'task_type': result['task_type'],
                'downstream_model': result['downstream_model']
            }

            # å¦‚æœæœ‰accuracyæ•°æ®ï¼Œæ·»åŠ åˆ°å­—å…¸ä¸­
            if len(values) > 3:
                step_dict['accuracy_metric'] = values[3]

            step_data.append(step_dict)

        # ä¿å­˜ä¸ºTSVæ ¼å¼
        df = pd.DataFrame(step_data)
        tsv_path = exp_dir / "process_results" / f"{model_name}_step_metrics.tsv"
        df.to_csv(tsv_path, sep='\t', index=False)
        print(f"ğŸ“Š æ­¥éª¤åº¦é‡æ•°æ®ä¿å­˜åˆ°: {tsv_path}")
        if accuracy_metrics:
            print(f"   åŒ…å« {len(step_data)} æ­¥æ•°æ®ï¼ŒåŒé‡å½’ä¸€åŒ–Information Measure + Accuracy Metric")
        else:
            print(f"   åŒ…å« {len(step_data)} æ­¥æ•°æ®ï¼ŒåŒé‡å½’ä¸€åŒ–Information Measure")

    def _save_summary_graphs(self, result: Dict):
        """ä¿å­˜ç®€åŒ–å›¾çš„ç¨€ç–è¡¨ç¤ºå’Œå¯è§†åŒ–"""
        exp_dir = result['exp_dir']
        model_name = result['model_name']
        summary_graphs = result['summary_graphs']
        dataset_name = result.get('dataset', '')

        # 0. ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯ï¼ˆæ ‡ç­¾å’Œæ•°æ®é›†åˆ’åˆ†ï¼‰- æ¯ä¸ªå®éªŒè®¾ç½®åªä¿å­˜ä¸€æ¬¡
        self._save_node_info(result)

        # 1. ä¿å­˜ç¨€ç–å›¾ (CSVæ ¼å¼)
        for step, graph in enumerate(summary_graphs):
            # æ£€æŸ¥æ˜¯å¦ä¸ºSO_relationæ•°æ®é›†ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ç‰¹æ®Šæ ¼å¼
            if dataset_name.startswith('SO_relation'):
                self._save_so_relation_format(graph, exp_dir, model_name, step, result)
            else:
                # æ ‡å‡†æ ¼å¼ï¼šä¿å­˜è¾¹åˆ—è¡¨
                edge_list = graph.edge_index.t().cpu().numpy()
                edge_df = pd.DataFrame(edge_list, columns=['source', 'target'])
                edge_df['step'] = step
                edge_df['model'] = model_name

                csv_path = exp_dir / "summary_graphs" / f"{model_name}_step_{step}_edges.csv"
                edge_df.to_csv(csv_path, index=False)

        print(f"ğŸ“ ç®€åŒ–å›¾CSVæ–‡ä»¶ä¿å­˜åˆ°: {exp_dir / 'summary_graphs'}")

        # 2. ç”Ÿæˆå¯è§†åŒ–å›¾ (PNGæ ¼å¼, æ— ä¸­æ–‡)
        self._visualize_summary_graphs(result)

    def _save_so_relation_format(self, graph: Data, exp_dir: Path, model_name: str, step: int, result: Dict):
        """ä¸ºSO_relationæ•°æ®é›†ä¿å­˜ç‰¹æ®Šæ ¼å¼çš„ç®€åŒ–å›¾"""
        original_graph = result['original_graph']
        dataset_name = result['dataset']

        # è·å–åŸå§‹å›¾çš„KOæ˜ å°„ä¿¡æ¯
        if hasattr(original_graph, '_idx_to_ko'):
            idx_to_ko = original_graph._idx_to_ko
        else:
            print(f"âš ï¸ åŸå§‹å›¾ç¼ºå°‘KOæ˜ å°„ä¿¡æ¯ï¼Œæ— æ³•ä¿å­˜SO_relationæ ¼å¼")
            return

        # è·å–è¾¹åˆ—è¡¨
        edge_index = graph.edge_index.cpu()
        edge_list = []

        # è½¬æ¢ä¸ºKOæ ¼å¼çš„è¾¹åˆ—è¡¨
        for i in range(edge_index.size(1)):
            src_idx = edge_index[0, i].item()
            tgt_idx = edge_index[1, i].item()

            # è½¬æ¢ç´¢å¼•ä¸ºKO ID
            if src_idx in idx_to_ko and tgt_idx in idx_to_ko:
                src_ko = idx_to_ko[src_idx]
                tgt_ko = idx_to_ko[tgt_idx]

                # é¿å…é‡å¤è¾¹ï¼ˆæ— å‘å›¾ï¼‰
                if src_ko < tgt_ko:  # æŒ‰å­—å…¸åºæ’åºé¿å…é‡å¤
                    edge_list.append((src_ko, tgt_ko, 1.0))  # æƒé‡è®¾ä¸º1.0ï¼ˆç®€åŒ–å›¾ä¸ºæ— æƒå›¾ï¼‰

        # åˆ›å»ºDataFrameï¼Œæ ¼å¼ä¸åŸå§‹æ•°æ®ä¸€è‡´
        if edge_list:
            edge_df = pd.DataFrame(edge_list, columns=['KO1', 'KO2', 'Weight'])
        else:
            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œåˆ›å»ºç©ºçš„DataFrameä½†ä¿æŒæ ¼å¼
            edge_df = pd.DataFrame(columns=['KO1', 'KO2', 'Weight'])

        # ä¿å­˜ä¸ºTSVæ ¼å¼ï¼ˆä¸åŸå§‹æ•°æ®æ ¼å¼ä¸€è‡´ï¼‰
        tsv_path = exp_dir / "summary_graphs" / f"{model_name}_step_{step}_ko_relation.tsv"
        edge_df.to_csv(tsv_path, sep='\t', index=False)

        print(f"ğŸ’¾ SO_relationæ ¼å¼ä¿å­˜: {tsv_path} ({len(edge_df)} edges)")

        # åŒæ—¶ä¿å­˜æ ‡å‡†æ ¼å¼ä»¥ä¾›å…¶ä»–åŠŸèƒ½ä½¿ç”¨
        edge_list_std = graph.edge_index.t().cpu().numpy()
        edge_df_std = pd.DataFrame(edge_list_std, columns=['source', 'target'])
        edge_df_std['step'] = step
        edge_df_std['model'] = model_name

        csv_path = exp_dir / "summary_graphs" / f"{model_name}_step_{step}_edges.csv"
        edge_df_std.to_csv(csv_path, index=False)

    def _save_node_info(self, result: Dict):
        """ä¿å­˜èŠ‚ç‚¹æ ‡ç­¾å’Œæ•°æ®é›†åˆ’åˆ†ä¿¡æ¯ï¼ˆæ¯ä¸ªå®éªŒè®¾ç½®åªä¿å­˜ä¸€æ¬¡ï¼‰"""
        exp_dir = result['exp_dir']
        model_name = result['model_name']
        original_graph = result['original_graph']

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆå› ä¸ºåŒä¸€ä¸ªå®éªŒè®¾ç½®çš„æ‰€æœ‰æ¨¡å‹å…±äº«ç›¸åŒçš„èŠ‚ç‚¹ä¿¡æ¯ï¼‰
        node_info_path = exp_dir / "summary_graphs" / "node_info.csv"
        if node_info_path.exists():
            return  # å·²ç»ä¿å­˜è¿‡ï¼Œæ— éœ€é‡å¤ä¿å­˜

        # è·å–èŠ‚ç‚¹æ ‡ç­¾
        if not hasattr(original_graph, 'y') or original_graph.y is None:
            print(f"âš ï¸ åŸå§‹å›¾ç¼ºå°‘æ ‡ç­¾ä¿¡æ¯ï¼Œè·³è¿‡èŠ‚ç‚¹ä¿¡æ¯ä¿å­˜")
            return

        labels = original_graph.y.cpu().numpy()
        num_nodes = original_graph.num_nodes

        # ä»resultä¸­è·å–train/val/test maskï¼ˆè¿™äº›æ˜¯åœ¨run_single_modelä¸­ä¿å­˜çš„ï¼‰
        # æ³¨æ„ï¼šè¿™äº›ä¿¡æ¯å­˜å‚¨åœ¨å®éªŒç›®å½•çº§åˆ«ï¼Œè€Œä¸æ˜¯åœ¨resultä¸­
        # æˆ‘ä»¬éœ€è¦ä»åŸå§‹çš„æ•°æ®é›†åŠ è½½ä¸­è·å–è¿™äº›mask
        # ç®€åŒ–å¤„ç†ï¼šæˆ‘ä»¬åœ¨è¿™é‡Œé‡æ–°åŠ è½½æ•°æ®é›†ä»¥è·å–mask
        try:
            from ..datasets import DatasetLoader
            dataset_loader = DatasetLoader(self.data_dir)
            dataset_name = result['dataset']
            task_type = result['task_type']

            # é‡æ–°åŠ è½½æ•°æ®é›†ä»¥è·å–mask
            _, train_mask, val_mask, test_mask = dataset_loader.load_dataset(
                dataset_name, task_type=task_type
            )

            # è½¬æ¢maskä¸ºå­—ç¬¦ä¸²æ ‡ç­¾
            split_labels = []
            for i in range(num_nodes):
                if train_mask[i]:
                    split_labels.append('train')
                elif val_mask[i]:
                    split_labels.append('val')
                elif test_mask[i]:
                    split_labels.append('test')
                else:
                    split_labels.append('unlabeled')  # å¦‚æœå­˜åœ¨æœªæ ‡è®°çš„èŠ‚ç‚¹

        except Exception as e:
            print(f"âš ï¸ æ— æ³•è·å–æ•°æ®é›†åˆ’åˆ†ä¿¡æ¯: {e}")
            # å¦‚æœæ— æ³•è·å–maskï¼Œä½¿ç”¨é»˜è®¤å€¼
            split_labels = ['unknown'] * num_nodes

        # åˆ›å»ºèŠ‚ç‚¹ä¿¡æ¯DataFrame
        node_data = {
            'node_id': list(range(num_nodes)),
            'label': labels.tolist(),
            'split': split_labels
        }

        node_df = pd.DataFrame(node_data)

        # ä¿å­˜ä¸ºCSV
        node_df.to_csv(node_info_path, index=False)
        print(f"ğŸ’¾ èŠ‚ç‚¹ä¿¡æ¯ä¿å­˜åˆ°: {node_info_path} ({num_nodes} nodes)")

    def _visualize_summary_graphs(self, result: Dict):
        """å¯è§†åŒ–ç®€åŒ–å›¾åºåˆ—ï¼Œä½¿ç”¨ä¸åŒé¢œè‰²æ ‡æ³¨ä¸åŒæ ‡ç­¾çš„èŠ‚ç‚¹ï¼Œæ˜¾ç¤ºaccuracy"""
        exp_dir = result['exp_dir']
        model_name = result['model_name']
        summary_graphs = result['summary_graphs']
        original_graph = result['original_graph']

        # å¦‚æœèŠ‚ç‚¹æ•°ç›®å¾ˆå¤šï¼Œè·³è¿‡å¯è§†åŒ–
        if original_graph.num_nodes > 200:
            print(f"âš ï¸ èŠ‚ç‚¹æ•°ç›®è¿‡å¤š ({original_graph.num_nodes} > 200)ï¼Œè·³è¿‡å›¾å¯è§†åŒ–")
            return

        # è·å–èŠ‚ç‚¹æ ‡ç­¾
        if not hasattr(original_graph, 'y') or original_graph.y is None:
            print(f"âš ï¸ åŸå§‹å›¾ç¼ºå°‘æ ‡ç­¾ä¿¡æ¯ï¼Œè·³è¿‡å›¾å¯è§†åŒ–")
            return

        node_labels = original_graph.y.cpu().numpy()
        unique_labels = np.unique(node_labels)
        num_labels = len(unique_labels)

        # ä¸ºä¸åŒæ ‡ç­¾åˆ†é…é¢œè‰²
        if num_labels <= 10:
            cmap = plt.cm.tab10
        elif num_labels <= 20:
            cmap = plt.cm.tab20
        else:
            cmap = plt.cm.hsv

        label_colors = {}
        for idx, label in enumerate(unique_labels):
            label_colors[label] = cmap(idx % cmap.N)

        # è·å–accuracy metricsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        accuracy_metrics = result.get('accuracy_metrics', None)

        # åŸºäºåŸå§‹å›¾è®¡ç®—å›ºå®šçš„èŠ‚ç‚¹ä½ç½®
        try:
            original_nx = to_networkx(original_graph, to_undirected=True)

            # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ç¡®ä¿å¸ƒå±€ä¸€è‡´æ€§
            np.random.seed(42)
            fixed_pos = nx.spring_layout(original_nx, k=1, iterations=100, seed=42)

        except Exception as e:
            print(f"âš ï¸ æ— æ³•è®¡ç®—å›ºå®šå¸ƒå±€: {e}")
            return

        # å¯¹æ¯ä¸€æ­¥éƒ½è¿›è¡Œå¯è§†åŒ–
        num_steps = len(summary_graphs)
        key_steps = list(range(num_steps))

        for step in key_steps:
            graph = summary_graphs[step]

            try:
                nx_graph = to_networkx(graph, to_undirected=True)

                plt.figure(figsize=(12, 9))

                # ä½¿ç”¨å›ºå®šä½ç½®
                current_nodes = set(nx_graph.nodes())

                # å‡†å¤‡æŒ‰æ ‡ç­¾åˆ†ç»„çš„èŠ‚ç‚¹
                all_nodes = set(fixed_pos.keys())
                connected_nodes = current_nodes
                isolated_nodes = all_nodes - connected_nodes

                # ç»˜åˆ¶æœ‰è¿æ¥çš„èŠ‚ç‚¹ï¼ˆæŒ‰æ ‡ç­¾ç€è‰²ï¼‰
                for label in unique_labels:
                    # æ‰¾åˆ°å±äºè¯¥æ ‡ç­¾ä¸”æœ‰è¿æ¥çš„èŠ‚ç‚¹
                    label_connected_nodes = [
                        node for node in connected_nodes
                        if node < len(node_labels) and node_labels[node] == label
                    ]

                    if label_connected_nodes:
                        label_pos = {node: fixed_pos[node] for node in label_connected_nodes}
                        nx.draw_networkx_nodes(
                            nx_graph, label_pos,
                            nodelist=label_connected_nodes,
                            node_size=50,
                            node_color=[label_colors[label]],
                            alpha=0.8,
                            label=f'Label {int(label)}'
                        )

                # ç»˜åˆ¶å­¤ç«‹èŠ‚ç‚¹ï¼ˆæŒ‰æ ‡ç­¾ç€è‰²ï¼Œä½†æ›´é€æ˜æ›´å°ï¼‰
                for label in unique_labels:
                    label_isolated_nodes = [
                        node for node in isolated_nodes
                        if node < len(node_labels) and node_labels[node] == label
                    ]

                    if label_isolated_nodes:
                        label_pos = {node: fixed_pos[node] for node in label_isolated_nodes}
                        nx.draw_networkx_nodes(
                            nx_graph, label_pos,
                            nodelist=label_isolated_nodes,
                            node_size=20,
                            node_color=[label_colors[label]],
                            alpha=0.3
                        )

                # ç»˜åˆ¶è¾¹
                if nx_graph.number_of_edges() > 0:
                    current_pos = {node: pos for node, pos in fixed_pos.items() if node in current_nodes}
                    nx.draw_networkx_edges(nx_graph, current_pos, alpha=0.4, width=0.8)

                # æ„å»ºæ ‡é¢˜ï¼ˆåŒ…å«accuracyä¿¡æ¯ï¼‰
                title = f'Step {step}: {original_graph.num_nodes} nodes, {nx_graph.number_of_edges()} edges'
                if accuracy_metrics is not None and step < len(accuracy_metrics):
                    accuracy = accuracy_metrics[step]
                    title += f', Accuracy: {accuracy:.4f}'

                plt.title(title, fontsize=14)
                plt.axis('off')

                # æ·»åŠ å›¾ä¾‹ï¼ˆæ˜¾ç¤ºæ ‡ç­¾é¢œè‰²ï¼‰
                if num_labels <= 10:  # åªåœ¨æ ‡ç­¾æ•°é‡ä¸å¤ªå¤šæ—¶æ˜¾ç¤ºå›¾ä¾‹
                    plt.legend(loc='upper right', fontsize=9, framealpha=0.9)

                # ä¿å­˜å›¾ç‰‡
                png_path = exp_dir / "graph_visualizations" / f"{model_name}_step_{step}_graph.png"
                plt.savefig(png_path, dpi=150, bbox_inches='tight')
                plt.close()

            except Exception as e:
                print(f"âš ï¸ æ­¥éª¤ {step} å¯è§†åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        print(f"ğŸ–¼ï¸ å›¾å¯è§†åŒ–ä¿å­˜åˆ°: {exp_dir / 'graph_visualizations'} (æŒ‰æ ‡ç­¾ç€è‰²ï¼Œæ˜¾ç¤ºaccuracy)")

    def _save_training_curves(self, result: Dict):
        """ä¿å­˜è®­ç»ƒæ›²çº¿"""
        exp_dir = result['exp_dir']
        model_name = result['model_name']
        training_history = result['training_history']

        if training_history is None:
            return

        try:
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            plt.figure(figsize=(12, 4))

            if isinstance(training_history, dict):
                # å¤„ç†ä¸åŒç±»å‹çš„è®­ç»ƒå†å²æ ¼å¼
                if 'train_loss' in training_history:
                    epochs = range(1, len(training_history['train_loss']) + 1)

                    plt.subplot(1, 2, 1)
                    plt.plot(epochs, training_history['train_loss'], 'b-', label='Train Loss')
                    if 'val_loss' in training_history:
                        plt.plot(epochs, training_history['val_loss'], 'r-', label='Validation Loss')
                    plt.xlabel('Epoch')
                    plt.ylabel('Loss')
                    plt.title('Training Loss Curve')
                    plt.legend()
                    plt.grid(True, alpha=0.3)

                    plt.subplot(1, 2, 2)
                    if 'train_acc' in training_history:
                        plt.plot(epochs, training_history['train_acc'], 'b-', label='Train Accuracy')
                    if 'val_acc' in training_history:
                        plt.plot(epochs, training_history['val_acc'], 'r-', label='Validation Accuracy')
                    plt.xlabel('Epoch')
                    plt.ylabel('Accuracy')
                    plt.title('Training Accuracy Curve')
                    plt.legend()
                    plt.grid(True, alpha=0.3)

                    plt.tight_layout()

                    # ä¿å­˜è®­ç»ƒæ›²çº¿å›¾
                    curve_path = exp_dir / "training_curves" / f"{model_name}_training_curves.png"
                    plt.savefig(curve_path, dpi=150, bbox_inches='tight')
                    plt.close()

                    print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿ä¿å­˜åˆ°: {curve_path}")

        except Exception as e:
            print(f"âš ï¸ è®­ç»ƒæ›²çº¿ä¿å­˜å¤±è´¥: {e}")

    def _plot_single_model_enhanced(self, result: Dict):
        """ç”Ÿæˆå¢å¼ºç‰ˆå•æ¨¡å‹ICæ›²çº¿å›¾"""
        exp_dir = result['exp_dir']
        model_name = result['model_name']
        model_info = result['model_info']
        complexity_metrics = result['complexity_metrics']
        information_metrics = result['information_metrics_additive']  # ä½¿ç”¨åŠ æ³•å½’ä¸€åŒ–

        plt.figure(figsize=(10, 6))

        # ä½¿ç”¨ç°æœ‰çš„å›¾æ ‡ç³»ç»Ÿ
        development_markers = {
            'gradient_based': ('o', '#1f77b4'),
            'neural_enhanced_main': ('s', '#ff7f0e'),
            'neural_enhanced_high_fusion': ('^', '#2ca02c'),
            'neural_enhanced_low_fusion': ('v', '#d62728'),
            'neural_enhanced_no_residual': ('D', '#9467bd'),
            'neural_enhanced_slow_gradient': ('*', '#8c564b'),
        }

        baseline_markers = {
            'networkit_forest_fire': ('o', '#e377c2'),
            'networkit_local_degree': ('s', '#ff7f0e'),
            'networkit_local_similarity': ('^', '#ffbb78'),
            'networkit_random_edge': ('v', '#2ECC71'),
            'networkit_random_node_edge': ('D', '#3498DB'),
            'networkit_scan': ('*', '#9467bd'),
            'networkit_simmelian': ('x', '#e377c2'),
            'pri_graphs': ('h', '#8c564b'),
        }

        # è·å–æ¨¡å‹çš„å›¾æ ‡å’Œé¢œè‰²
        if model_info['category'] == 'development':
            if model_name in development_markers:
                marker, color = development_markers[model_name]
            else:
                marker, color = 'o', '#1f77b4'  # Default: circle, blue
            linestyle = '-'
        elif model_info['category'] == 'baseline':
            if model_name in baseline_markers:
                marker, color = baseline_markers[model_name]
            else:
                marker, color = 's', '#E74C3C'  # Default: square, red
            linestyle = '--'
        else:
            marker, color = '^', 'green'  # Default: triangle, green
            linestyle = ':'

        # ç»˜åˆ¶ICæ›²çº¿
        plt.plot(
            complexity_metrics,
            information_metrics,
            marker=marker,
            color=color,
            linewidth=2,
            markersize=8,
            label=f"{model_name} (IC-AUC: {result['ic_auc_additive']:.3f})",
            linestyle=linestyle
        )

        plt.xlabel('Complexity Metric (Normalized Edge Count)', fontsize=12)
        plt.ylabel('Information Metric (Additive Normalization)', fontsize=12)
        plt.title(f'IC Curve - {model_name}\n{result["dataset"]} + {result["downstream_model"].upper()} ({result["task_type"]} task)', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=11)

        # ä¿å­˜åˆ°å®éªŒç›®å½•
        plot_path = exp_dir / "process_results" / f"{model_name}_ic_curve.png"
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ICæ›²çº¿å›¾ä¿å­˜åˆ°: {plot_path}")

    def _save_comprehensive_results(self, results: List[Dict], exp_dir: Path):
        """ä¿å­˜ç»¼åˆç»“æœ"""
        # 1. ç”ŸæˆåŠ æ³•å½’ä¸€åŒ–ICæ›²çº¿å¯¹æ¯”å›¾ï¼ˆé»˜è®¤ä¼˜å…ˆçº§ï¼‰
        self._plot_comprehensive_ic_curves(results, exp_dir, 'additive')

        # 2. ç”Ÿæˆå¯¹æ•°æ¯”ç‡å½’ä¸€åŒ–ICæ›²çº¿å¯¹æ¯”å›¾
        self._plot_comprehensive_ic_curves(results, exp_dir, 'log_ratio')

        # 3. ä¿å­˜IC-AUCå¯¹æ¯”è¡¨ (TSVæ ¼å¼ï¼ŒåŒ…å«ä¸¤ç§å½’ä¸€åŒ–)
        self._save_ic_auc_table(results, exp_dir)

        # 4. ä¿å­˜ä¿¡æ¯é˜ˆå€¼ç‚¹å¯¹æ¯”è¡¨
        self._save_threshold_point_table(results, exp_dir)

        print(f"âœ… ç»¼åˆç»“æœå·²ä¿å­˜åˆ°: {exp_dir / 'comprehensive_results'}")

    def _plot_comprehensive_ic_curves(self, results: List[Dict], exp_dir: Path, normalization: str = 'log_ratio'):
        """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„ICæ›²çº¿å¯¹æ¯”å›¾"""
        plt.figure(figsize=(16, 12))

        # é€‰æ‹©ä¿¡æ¯åº¦é‡æ•°æ®
        if normalization == 'log_ratio':
            info_key = 'information_metrics_log_ratio'
            title_suffix = 'Log-ratio Normalization'
            ylabel = 'Information Metric (Log-ratio Normalization)'
        else:
            info_key = 'information_metrics_additive'
            title_suffix = 'Additive Normalization'
            ylabel = 'Information Metric (Additive Normalization)'

        # ä½¿ç”¨æ›´æ–°çš„å›¾æ ‡ç³»ç»Ÿ
        development_markers = {
            'gradient_based': ('o', '#1f77b4'),
            'neural_enhanced_main': ('s', '#ff7f0e'),
            'neural_enhanced_high_fusion': ('^', '#2ca02c'),
            'neural_enhanced_low_fusion': ('v', '#d62728'),
            'neural_enhanced_no_residual': ('D', '#9467bd'),
            'neural_enhanced_slow_gradient': ('*', '#8c564b'),
        }

        baseline_markers = {
            'networkit_forest_fire': ('o', '#e377c2'),
            'networkit_local_degree': ('s', '#ff7f0e'),
            'networkit_local_similarity': ('^', '#ffbb78'),
            'networkit_random_edge': ('v', '#2ECC71'),
            'networkit_random_node_edge': ('D', '#3498DB'),
            'networkit_scan': ('*', '#9467bd'),
            'networkit_simmelian': ('x', '#e377c2'),
            'pri_graphs': ('h', '#8c564b'),
        }

        for result in results:
            model_name = result['model_name']
            category = result['model_info']['category']

            # è·å–å¯¹åº”å½’ä¸€åŒ–æ–¹å¼çš„AUC
            if normalization == 'log_ratio':
                ic_auc = result['ic_auc_log_ratio']
            else:
                ic_auc = result['ic_auc_additive']

            # è®¾ç½®å›¾æ ‡å’Œé¢œè‰²
            if category == 'development':
                if model_name in development_markers:
                    marker, color = development_markers[model_name]
                    linestyle = '-'
                else:
                    # Fallback for unknown development models
                    color = '#1f77b4'  # Default blue
                    marker = 'o'
                    linestyle = '-'

            elif category == 'baseline':
                if model_name in baseline_markers:
                    marker, color = baseline_markers[model_name]
                    linestyle = '--'
                else:
                    # Fallback for unknown baseline models
                    color = '#E74C3C'  # Default red
                    marker = 's'
                    linestyle = '--'
            else:
                # Unknown category
                color = 'green'
                marker = '^'
                linestyle = ':'

            plt.plot(
                result['complexity_metrics'],
                result[info_key],
                marker=marker,
                linestyle=linestyle,
                color=color,
                linewidth=2,
                markersize=8,
                alpha=0.8,
                label=f"{model_name} ({category}, AUC={ic_auc:.3f})"
            )

        # è·å–å®éªŒé…ç½®ä¿¡æ¯ï¼ˆä¼˜å…ˆä»experiment_configsï¼Œå¦åˆ™ä»ç»“æœä¸­è·å–ï¼‰
        if hasattr(self, 'experiment_configs') and result['exp_id'] in self.experiment_configs:
            exp_config = self.experiment_configs[result['exp_id']]
            dataset = exp_config["dataset"]
            downstream_model = exp_config["downstream_model"]
            task_type = exp_config["task_type"]
        else:
            # ä»ç»“æœä¸­ç›´æ¥è·å–
            dataset = result['dataset']
            downstream_model = result['downstream_model']
            task_type = result['task_type']

        plt.xlabel('Complexity Metric (Normalized Edge Count)', fontsize=12)
        plt.ylabel(ylabel, fontsize=12)
        plt.title(f'IC Curve Comparison ({title_suffix}) - {dataset} + {downstream_model.upper()} ({task_type} task)', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        plot_path = exp_dir / "comprehensive_results" / f"ic_curves_comparison_{normalization}.png"
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ICæ›²çº¿å¯¹æ¯”å›¾ï¼ˆ{title_suffix}ï¼‰ä¿å­˜åˆ°: {plot_path}")

    def _save_ic_auc_table(self, results: List[Dict], exp_dir: Path):
        """ä¿å­˜IC-AUCå¯¹æ¯”è¡¨ï¼ˆåŒ…å«ä¸¤ç§å½’ä¸€åŒ–ï¼‰"""
        summary_data = []
        for result in results:
            summary_data.append({
                'model': result['model_name'],
                'category': result['model_info']['category'],
                'description': result['model_info']['description'],
                'ic_auc_log_ratio': result['ic_auc_log_ratio'],
                'ic_auc_additive': result['ic_auc_additive'],
                'threshold_point_log_ratio': result.get('threshold_point_log_ratio', None),
                'threshold_point_additive': result.get('threshold_point_additive', None),
                'training_time_seconds': result['training_time'],
                'summarization_time_seconds': result['summarization_time'],
                'dataset': result['dataset'],
                'task_type': result['task_type'],
                'downstream_model': result['downstream_model']
            })

        df_summary = pd.DataFrame(summary_data)
        df_summary = df_summary.sort_values('ic_auc_additive', ascending=False)

        tsv_path = exp_dir / "comprehensive_results" / "ic_auc_comparison.tsv"
        df_summary.to_csv(tsv_path, sep='\t', index=False)
        print(f"ğŸ“‹ IC-AUCå¯¹æ¯”è¡¨ä¿å­˜åˆ°: {tsv_path}")

    def _save_threshold_point_table(self, results: List[Dict], exp_dir: Path):
        """ä¿å­˜ä¿¡æ¯é˜ˆå€¼ç‚¹å¯¹æ¯”è¡¨"""
        threshold_data = []
        for result in results:
            threshold_data.append({
                'model': result['model_name'],
                'category': result['model_info']['category'],
                'threshold_point_log_ratio': result.get('threshold_point_log_ratio', None),
                'threshold_point_additive': result.get('threshold_point_additive', None),
                'dataset': result['dataset'],
                'task_type': result['task_type'],
                'downstream_model': result['downstream_model']
            })

        df_threshold = pd.DataFrame(threshold_data)
        # æŒ‰ç…§é˜ˆå€¼ç‚¹æ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼ŒNoneå€¼æ’åœ¨æœ€åï¼‰
        df_threshold['threshold_sort'] = df_threshold['threshold_point_log_ratio'].fillna(float('inf'))
        df_threshold = df_threshold.sort_values('threshold_sort', ascending=True)
        df_threshold = df_threshold.drop('threshold_sort', axis=1)

        tsv_path = exp_dir / "comprehensive_results" / "threshold_points_comparison.tsv"
        df_threshold.to_csv(tsv_path, sep='\t', index=False)
        print(f"ğŸ“‹ ä¿¡æ¯é˜ˆå€¼ç‚¹å¯¹æ¯”è¡¨ä¿å­˜åˆ°: {tsv_path}")

    def _save_detailed_results(self, results: List[Dict], dataset_name: str, downstream_model: str):
        """ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV"""
        detailed_data = []
        for result in results:
            for i, (complexity, information) in enumerate(zip(
                result['complexity_metrics'], 
                result['information_metrics']
            )):
                detailed_data.append({
                    'model': result['model_name'],
                    'category': result['model_info']['category'],
                    'dataset': dataset_name,
                    'downstream': downstream_model,
                    'step': i,
                    'complexity': complexity,
                    'information': information,
                    'snr_auc': result['snr_auc']
                })
        
        df = pd.DataFrame(detailed_data)
        csv_path = self.results_dir / f'detailed_comparison_{dataset_name}_{downstream_model}.csv'
        df.to_csv(csv_path, index=False)
        print(f"è¯¦ç»†ç»“æœä¿å­˜åˆ°: {csv_path}")
    
    def _generate_comparison_report(self, results: List[Dict], dataset_name: str, downstream_model: str):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        summary_data = []
        for result in results:
            summary_data.append({
                'model': result['model_name'],
                'category': result['model_info']['category'],
                'description': result['model_info']['description'],
                'snr_auc': result['snr_auc'],
                'time_seconds': result['summarization_time']
            })
        
        df_summary = pd.DataFrame(summary_data)
        df_summary = df_summary.sort_values('snr_auc', ascending=False)
        
        tsv_path = self.results_dir / f'model_comparison_{dataset_name}_{downstream_model}.tsv'
        df_summary.to_csv(tsv_path, sep='\t', index=False)
        print(f"å¯¹æ¯”æŠ¥å‘Šä¿å­˜åˆ°: {tsv_path}")
        
        # æ‰“å°æ’å
        print(f"\n{'='*100}")
        print(f"{dataset_name} + {downstream_model.upper()} æ¨¡å‹æ€§èƒ½æ’å")
        print(f"{'='*100}")
        print(f"{'æ’å':<4} {'æ¨¡å‹':<20} {'ç±»åˆ«':<12} {'SNR-AUC':<10} {'æ—¶é—´(s)':<8}")
        print("-" * 60)
        
        for i, row in df_summary.iterrows():
            print(f"{i+1:<4} {row['model']:<20} {row['category']:<12} {row['snr_auc']:<10.2f} {row['time_seconds']:<8.2f}")
    
    def _plot_comparison(self, results: List[Dict], dataset_name: str, downstream_model: str):
        """ç»˜åˆ¶å¯¹æ¯”å›¾"""
        plt.figure(figsize=(16, 12))
        
        # Define distinct markers for all development models
        development_markers = {
            'learnable': ('o', '#1f77b4'),           # Circle, blue
            'learnable_main': ('s', '#ff7f0e'),      # Square, orange
            'learnable_gat': ('^', '#2ca02c'),       # Triangle up, green
            'learnable_sage': ('v', '#d62728'),      # Triangle down, red
            'learnable_no_step_emb': ('D', '#9467bd'), # Diamond, purple
            'learnable_no_edge_diff': ('*', '#8c564b'), # Star, brown
            'learnable_small_hidden': ('h', '#e377c2'), # Hexagon, pink
            'learnable_large_hidden': ('p', '#7f7f7f'), # Pentagon, gray
            'learnable_deep_gin': ('H', '#bcbd22'),   # Hexagon2, olive
            'learnable_shallow_gin': ('+', '#17becf'), # Plus, cyan
            'learnable_fixed_uniform': ('x', '#ff1744'), # X, deep red
            'learnable_fixed_cosine': ('|', '#00e676'),  # Vline, light green
            'learnable_dynamic_fw': ('_', '#3f51b5'),    # Hline, indigo
            'learnable_dynamic_ugd': ('1', '#ff5722'),   # Tri_down, deep orange
            'gradient_based': ('*', '#FF6B35'),          # Star, bright orange - åŸºäºæ¢¯åº¦çš„æ¨¡å‹
        }
        
        # Define distinct markers for baseline models  
        baseline_markers = {
            'networkit_forest_fire': ('o', '#E74C3C'),      # Circle, red
            'networkit_local_degree': ('s', '#E67E22'),      # Square, orange  
            'networkit_local_similarity': ('^', '#F1C40F'),  # Triangle up, yellow
            'networkit_random_edge': ('v', '#2ECC71'),       # Triangle down, green
            'networkit_random_node_edge': ('D', '#3498DB'),  # Diamond, blue
            'networkit_scan': ('*', '#9B59B6'),              # Star, purple
            'networkit_simmelian': ('X', '#E91E63'),         # X, pink
            'pri_graphs': ('h', '#795548'),                  # Hexagon, brown
        }
        
        for result in results:
            model_name = result['model_name']
            category = result['model_info']['category']
            snr_auc = result['snr_auc']
            
            # Set colors and markers based on specific model
            if category == 'development':
                if model_name in development_markers:
                    marker, color = development_markers[model_name]
                    linestyle = '-'
                else:
                    # Fallback for unknown development models
                    color = '#1f77b4'  # Default blue
                    marker = 'o'
                    linestyle = '-'
                    
            elif category == 'baseline':
                if model_name in baseline_markers:
                    marker, color = baseline_markers[model_name]
                    linestyle = '--'
                else:
                    # Fallback for unknown baseline models
                    color = '#E74C3C'  # Default red
                    marker = 's'
                    linestyle = '--'
            else:
                # Unknown category
                color = 'green'
                marker = '^'
                linestyle = ':'
            
            plt.plot(
                result['complexity_metrics'], 
                result['information_metrics'],
                marker=marker,
                linestyle=linestyle,
                color=color,
                linewidth=2,
                markersize=8,
                alpha=0.8,  # Add transparency for better visualization
                label=f"{model_name} ({category}, AUC={snr_auc:.1f})"
            )
        
        plt.xlabel('Complexity Metric (L0 Norm)', fontsize=12)
        plt.ylabel('Information Metric', fontsize=12) 
        plt.title(f'Model Comparison - {dataset_name} + {downstream_model.upper()}', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        plot_path = self.results_dir / f'model_comparison_{dataset_name}_{downstream_model}.png'
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"å¯¹æ¯”å›¾ä¿å­˜åˆ°: {plot_path}")
    
    def _plot_single_model(self, result: Dict, dataset_name: str, downstream_model: str):
        """ä¸ºå•ä¸ªæ¨¡å‹ç”ŸæˆSNRæ›²çº¿å›¾"""
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(10, 6))
        
        model_name = result['model_name']
        model_info = result['model_info']
        complexity_metrics = result['complexity_metrics']
        information_metrics = result['information_metrics']
        
        # å®šä¹‰å›¾æ ‡å’Œé¢œè‰²
        development_markers = {
            'learnable': ('o', '#1f77b4'),           # Circle, blue
            'learnable_main': ('s', '#ff7f0e'),      # Square, orange
            'learnable_gat': ('^', '#2ca02c'),       # Triangle up, green
            'learnable_sage': ('v', '#d62728'),      # Triangle down, red
            'learnable_no_step_emb': ('D', '#9467bd'), # Diamond, purple
            'learnable_no_edge_diff': ('*', '#8c564b'), # Star, brown
            'learnable_small_hidden': ('h', '#e377c2'), # Hexagon, pink
            'learnable_large_hidden': ('p', '#7f7f7f'), # Pentagon, gray
            'learnable_deep_gin': ('H', '#bcbd22'),   # Hexagon2, olive
            'learnable_shallow_gin': ('+', '#17becf'), # Plus, cyan
            'learnable_fixed_uniform': ('x', '#ff1744'), # X, deep red
            'learnable_fixed_cosine': ('|', '#00e676'),  # Vline, light green
            'learnable_dynamic_fw': ('_', '#3f51b5'),    # Hline, indigo
            'learnable_dynamic_ugd': ('1', '#ff5722'),   # Tri_down, deep orange
            'gradient_based': ('*', '#FF6B35'),          # Star, bright orange - åŸºäºæ¢¯åº¦çš„æ¨¡å‹
        }
        
        baseline_markers = {
            'networkit_forest_fire': ('o', '#E74C3C'),      # Circle, red
            'networkit_local_degree': ('s', '#E67E22'),      # Square, orange  
            'networkit_local_similarity': ('^', '#F1C40F'),  # Triangle up, yellow
            'networkit_random_edge': ('v', '#2ECC71'),       # Triangle down, green
            'networkit_random_node_edge': ('D', '#3498DB'),  # Diamond, blue
            'networkit_scan': ('*', '#9B59B6'),              # Star, purple
            'networkit_simmelian': ('X', '#E91E63'),         # X, pink
            'pri_graphs': ('h', '#795548'),                  # Hexagon, brown
        }
        
        # è·å–æ¨¡å‹çš„å›¾æ ‡å’Œé¢œè‰²
        if model_info['category'] == 'development':
            if model_name in development_markers:
                marker, color = development_markers[model_name]
            else:
                marker, color = 'o', '#1f77b4'  # Default: circle, blue
        elif model_info['category'] == 'baseline':
            if model_name in baseline_markers:
                marker, color = baseline_markers[model_name]
            else:
                marker, color = 's', '#E74C3C'  # Default: square, red
        else:
            marker, color = '^', 'green'  # Default: triangle, green
        
        # ç»˜åˆ¶SNRæ›²çº¿
        plt.plot(
            complexity_metrics, 
            information_metrics, 
            marker=marker,
            color=color,
            linewidth=2,
            markersize=8,
            label=f"{model_name} (SNR-AUC: {result['snr_auc']:.2f})",
            linestyle='--' if model_info['category'] == 'baseline' else '-'
        )
        
        plt.xlabel('Complexity Metric (Edge Count)', fontsize=12)
        plt.ylabel('Information Metric', fontsize=12)
        plt.title(f'SNR Curve - {model_name} on {dataset_name} + {downstream_model.upper()}', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=11)
        
        # åè½¬xè½´ï¼Œå› ä¸ºå¤æ‚åº¦ä»é«˜åˆ°ä½
        plt.gca().invert_xaxis()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = self.results_dir / f'single_model_{model_name}_{dataset_name}_{downstream_model}.png'
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š å•æ¨¡å‹SNRæ›²çº¿å›¾ä¿å­˜åˆ°: {plot_path}")
    
    def list_available_models(self) -> Dict[str, List[str]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        dev_models = model_registry.list_development_models()
        baseline_models = model_registry.list_baseline_models()
        
        print(f"\nå¯ç”¨çš„Graph Summarizationæ¨¡å‹:")
        print(f"  å¼€å‘æ¨¡å‹ ({len(dev_models)} ä¸ª): {', '.join(dev_models)}")
        print(f"  åŸºå‡†æ¨¡å‹ ({len(baseline_models)} ä¸ª): {', '.join(baseline_models)}")
        
        return {
            'development': dev_models,
            'baseline': baseline_models
        }

    def run_multi_task_benchmark(self,
                                 model_names: List[str],
                                 dataset_names: List[str] = None,
                                 task_types: List[str] = None,
                                 downstream_model: str = 'gcn',
                                 num_steps: int = 10,
                                 epochs: int = 100) -> Dict[str, Any]:
        """
        è¿è¡Œå¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œå¯¹å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ç±»å‹è¿›è¡Œæµ‹è¯•

        Args:
            model_names: è¦æµ‹è¯•çš„æ¨¡å‹åç§°åˆ—è¡¨
            dataset_names: æ•°æ®é›†åç§°åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†
            task_types: ä»»åŠ¡ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ['original', 'degree']
            downstream_model: ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹ç±»å‹
            num_steps: å›¾æ€»ç»“æ­¥æ•°
            epochs: è®­ç»ƒè½®æ•°

        Returns:
            åŒ…å«æ‰€æœ‰æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        if dataset_names is None:
            # æ’é™¤å¤ªå¤§çš„æ•°æ®é›†
            dataset_names = ['Cora', 'CiteSeer', 'PubMed', 'KarateClub']

        if task_types is None:
            task_types = ['original', 'degree']

        print(f"\n{'='*120}")
        print(f"å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•")
        print(f"æ¨¡å‹: {model_names}")
        print(f"æ•°æ®é›†: {dataset_names}")
        print(f"ä»»åŠ¡ç±»å‹: {task_types}")
        print(f"{'='*120}")

        all_results = {}
        summary_data = []

        total_experiments = len(model_names) * len(dataset_names) * len(task_types)
        current_experiment = 0

        for dataset_name in dataset_names:
            for task_type in task_types:
                experiment_key = f"{dataset_name}_{task_type}"
                print(f"\n{'='*80}")
                print(f"å®éªŒè®¾ç½®: {dataset_name} + {task_type} ä»»åŠ¡")
                print(f"{'='*80}")

                experiment_results = []
                for model_name in model_names:
                    current_experiment += 1
                    print(f"\n[{current_experiment}/{total_experiments}] æµ‹è¯• {model_name}...")

                    try:
                        result = self.run_single_model(
                            model_name=model_name,
                            dataset_name=dataset_name,
                            task_type=task_type,
                            downstream_model=downstream_model,
                            num_steps=num_steps,
                            epochs=epochs
                        )

                        if result.get('success', False):
                            experiment_results.append(result)
                            summary_data.append({
                                'dataset': dataset_name,
                                'task': task_type,
                                'model': model_name,
                                'category': result['model_info']['category'],
                                'snr_auc': result['snr_auc'],
                                'training_time': result.get('training_time', 0),
                                'summarization_time': result.get('summarization_time', 0)
                            })
                        else:
                            print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥")

                    except Exception as e:
                        print(f"âŒ {model_name} æµ‹è¯•å‡ºé”™: {e}")

                # ä¿å­˜å½“å‰å®éªŒè®¾ç½®çš„ç»“æœ
                if experiment_results:
                    all_results[experiment_key] = {
                        'results': experiment_results,
                        'dataset': dataset_name,
                        'task_type': task_type
                    }

                    # ç”Ÿæˆå½“å‰å®éªŒçš„å¯¹æ¯”å›¾
                    self._plot_comparison(experiment_results, f"{dataset_name}_{task_type}", downstream_model)

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_multi_task_report(summary_data, downstream_model)

        return {
            'all_results': all_results,
            'summary': summary_data,
            'total_experiments': total_experiments,
            'success_rate': len(summary_data) / total_experiments if total_experiments > 0 else 0
        }

    def _generate_multi_task_report(self, summary_data: List[Dict], downstream_model: str):
        """
        ç”Ÿæˆå¤šä»»åŠ¡æµ‹è¯•çš„ç»¼åˆæŠ¥å‘Š
        """
        if not summary_data:
            print("æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return

        df = pd.DataFrame(summary_data)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_path = self.results_dir / f'multi_task_detailed_{downstream_model}.csv'
        df.to_csv(detailed_path, index=False)
        print(f"\nğŸ“Š è¯¦ç»†å¤šä»»åŠ¡ç»“æœä¿å­˜åˆ°: {detailed_path}")

        # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
        pivot_table = df.pivot_table(
            values='snr_auc',
            index=['model', 'category'],
            columns=['dataset', 'task'],
            aggfunc='mean'
        )

        summary_path = self.results_dir / f'multi_task_summary_{downstream_model}.tsv'
        pivot_table.to_csv(summary_path, sep='\t')
        print(f"ğŸ“Š å¤šä»»åŠ¡æ±‡æ€»è¡¨ä¿å­˜åˆ°: {summary_path}")

        # æ‰“å°æ±‡æ€»ç»Ÿè®¡
        print(f"\n{'='*120}")
        print(f"å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•æ±‡æ€» ({downstream_model.upper()})")
        print(f"{'='*120}")

        # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
        model_stats = df.groupby(['model', 'category']).agg({
            'snr_auc': ['mean', 'std', 'count'],
            'training_time': 'mean',
            'summarization_time': 'mean'
        }).round(4)

        print("\næ¨¡å‹æ€§èƒ½ç»Ÿè®¡:")
        print(model_stats)

        # æœ€ä½³æ¨¡å‹ç»Ÿè®¡
        best_by_task = df.loc[df.groupby(['dataset', 'task'])['snr_auc'].idxmax()]
        print("\nå„ä»»åŠ¡æœ€ä½³æ¨¡å‹:")
        for _, row in best_by_task.iterrows():
            print(f"  {row['dataset']} + {row['task']}: {row['model']} (SNR-AUC: {row['snr_auc']:.4f})")

    def compute_comprehensive_results_from_process_data(self,
                                                       experiment_dir: str,
                                                       model_names: List[str] = None) -> Dict[str, Any]:
        """
        å•ç‹¬çš„ç»¼åˆç»“æœè®¡ç®—æ¥å£ï¼šæ ¹æ®å·²ä¿å­˜çš„è¿‡ç¨‹ç»“æœè®¡ç®—ç»¼åˆç»“æœ

        è¿™ä¸ªæ–¹æ³•ç¬¦åˆDEMAND.mdçš„è¦æ±‚ï¼Œç”¨äºåœ¨æ‰€æœ‰æ¨¡å‹çš„è¿‡ç¨‹ç»“æœéƒ½å®Œæˆåï¼Œ
        å•ç‹¬è®¡ç®—ç»¼åˆç»“æœï¼ˆSNRæ›²çº¿å¯¹æ¯”å›¾å’ŒSNR-AUCè¡¨æ ¼ï¼‰

        Args:
            experiment_dir: å®éªŒç›®å½•è·¯å¾„ (å¦‚ "results/comprehensive_benchmark/Cora_original_gcn")
            model_names: è¦åŒ…å«çš„æ¨¡å‹åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŒ…å«æ‰€æœ‰æ‰¾åˆ°çš„æ¨¡å‹

        Returns:
            ç»¼åˆç»“æœå­—å…¸
        """
        exp_dir = Path(experiment_dir)

        if not exp_dir.exists():
            raise ValueError(f"å®éªŒç›®å½•ä¸å­˜åœ¨: {experiment_dir}")

        process_dir = exp_dir / "process_results"
        if not process_dir.exists():
            raise ValueError(f"è¿‡ç¨‹ç»“æœç›®å½•ä¸å­˜åœ¨: {process_dir}")

        print(f"ğŸ”„ ä»è¿‡ç¨‹ç»“æœè®¡ç®—ç»¼åˆç»“æœ: {experiment_dir}")

        # æ‰«ææ‰€æœ‰å¯ç”¨çš„è¿‡ç¨‹ç»“æœæ–‡ä»¶
        available_models = set()
        for file in process_dir.glob("*_step_metrics.tsv"):
            model_name = file.stem.replace("_step_metrics", "")
            available_models.add(model_name)

        if not available_models:
            raise ValueError(f"åœ¨ {process_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•è¿‡ç¨‹ç»“æœæ–‡ä»¶")

        # è¿‡æ»¤æ¨¡å‹
        if model_names is not None:
            available_models = available_models.intersection(set(model_names))
            missing_models = set(model_names) - available_models
            if missing_models:
                print(f"âš ï¸ ä»¥ä¸‹æ¨¡å‹çš„è¿‡ç¨‹ç»“æœæœªæ‰¾åˆ°: {missing_models}")

        if not available_models:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿‡ç¨‹ç»“æœ")

        print(f"ğŸ“Š æ‰¾åˆ° {len(available_models)} ä¸ªæ¨¡å‹çš„è¿‡ç¨‹ç»“æœ: {list(available_models)}")

        # ä»è¿‡ç¨‹ç»“æœæ–‡ä»¶åŠ è½½æ•°æ®
        results = []
        for model_name in available_models:
            try:
                # è¯»å–æ­¥éª¤åº¦é‡æ•°æ®
                step_metrics_file = process_dir / f"{model_name}_step_metrics.tsv"
                df = pd.read_csv(step_metrics_file, sep='\t')

                complexity_metrics = df['complexity_metric'].tolist()

                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„åŒé‡å½’ä¸€åŒ–æŒ‡æ ‡
                if 'information_metric_log_ratio' in df.columns and 'information_metric_additive' in df.columns:
                    # æ–°æ ¼å¼ï¼šæœ‰åŒé‡å½’ä¸€åŒ–
                    information_metrics_log = df['information_metric_log_ratio'].tolist()
                    information_metrics_add = df['information_metric_additive'].tolist()
                    information_metrics = information_metrics_add  # å…¼å®¹æ—§å­—æ®µï¼Œé»˜è®¤ä½¿ç”¨åŠ æ³•å½’ä¸€åŒ–
                else:
                    # æ—§æ ¼å¼ï¼šåªæœ‰å•ä¸€å½’ä¸€åŒ–ï¼ˆå‡è®¾ä¸ºåŠ æ³•å½’ä¸€åŒ–ï¼‰
                    information_metrics = df['information_metric'].tolist()
                    information_metrics_add = information_metrics
                    information_metrics_log = information_metrics  # ç”¨ç›¸åŒå€¼å¡«å……

                # è®¡ç®—IC-AUCï¼ˆä¸¤ç§å½’ä¸€åŒ–ï¼‰
                from ..metrics import ICAnalysis
                ic_auc_log = ICAnalysis.compute_ic_auc(complexity_metrics, information_metrics_log)
                ic_auc_add = ICAnalysis.compute_ic_auc(complexity_metrics, information_metrics_add)

                # è®¡ç®—ä¿¡æ¯é˜ˆå€¼ç‚¹
                threshold_point_log = ICAnalysis.compute_information_threshold_point(
                    complexity_metrics, information_metrics_log, threshold=0.8)
                threshold_point_add = ICAnalysis.compute_information_threshold_point(
                    complexity_metrics, information_metrics_add, threshold=0.8)

                # å‘åå…¼å®¹
                snr_auc = ic_auc_add

                # å°è¯•è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                model_category = 'unknown'
                model_description = f"Model {model_name}"

                # åŸºäºæ¨¡å‹åç§°æ¨æ–­ç±»åˆ«
                if 'networkit' in model_name or 'pri_graphs' in model_name:
                    model_category = 'baseline'
                elif 'neural_enhanced' in model_name or 'gradient_based' in model_name:
                    model_category = 'development'

                result = {
                    'model_name': model_name,
                    'model_info': {
                        'category': model_category,
                        'description': model_description
                    },
                    'complexity_metrics': complexity_metrics,
                    # åŒé‡å½’ä¸€åŒ–ä¿¡æ¯åº¦é‡
                    'information_metrics_log_ratio': information_metrics_log,
                    'information_metrics_additive': information_metrics_add,
                    # IC-AUCæŒ‡æ ‡ï¼ˆä¸¤ç§å½’ä¸€åŒ–ï¼‰
                    'ic_auc_log_ratio': ic_auc_log,
                    'ic_auc_additive': ic_auc_add,
                    # ä¿¡æ¯é˜ˆå€¼ç‚¹ï¼ˆä¸¤ç§å½’ä¸€åŒ–ï¼‰
                    'threshold_point_log_ratio': threshold_point_log,
                    'threshold_point_additive': threshold_point_add,
                    # å‘åå…¼å®¹çš„å­—æ®µ
                    'information_metrics': information_metrics,
                    'snr_auc': snr_auc,
                    # é»˜è®¤å€¼ï¼ˆä»è¿‡ç¨‹æ•°æ®é‡å»ºæ—¶ä¸å¯ç”¨ï¼‰
                    'training_time': 0.0,
                    'summarization_time': 0.0,
                    'training_history': None,
                    'summary_graphs': None,
                    'original_graph': None,
                    'success': True,
                    'loaded_from_process_data': True
                }

                results.append(result)
                print(f"  âœ… {model_name}: IC-AUC(log)={ic_auc_log:.4f}, IC-AUC(add)={ic_auc_add:.4f}")

            except Exception as e:
                print(f"  âŒ æ— æ³•åŠ è½½ {model_name} çš„è¿‡ç¨‹ç»“æœ: {e}")
                continue

        if not results:
            raise ValueError("æ— æ³•åŠ è½½ä»»ä½•æœ‰æ•ˆçš„è¿‡ç¨‹ç»“æœ")

        # åˆ›å»ºcomprehensive_resultsç›®å½•
        comprehensive_dir = exp_dir / "comprehensive_results"
        comprehensive_dir.mkdir(exist_ok=True)

        # è§£æå®éªŒä¿¡æ¯ï¼ˆä»ç›®å½•åï¼‰
        exp_parts = exp_dir.name.split('_')
        if len(exp_parts) >= 3:
            dataset_name = exp_parts[0]
            task_type = exp_parts[1]
            downstream_model = exp_parts[2]
        else:
            dataset_name = "Unknown"
            task_type = "unknown"
            downstream_model = "unknown"

        # ä¸ºç»“æœæ·»åŠ å®éªŒä¿¡æ¯
        for result in results:
            result['dataset'] = dataset_name
            result['task_type'] = task_type
            result['downstream_model'] = downstream_model
            result['exp_dir'] = exp_dir
            result['exp_id'] = exp_dir.name

        # ç”Ÿæˆç»¼åˆç»“æœ
        self._save_comprehensive_results(results, exp_dir)

        # ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Šï¼ˆå¯é€‰åŠŸèƒ½ï¼Œå¿½ç•¥é”™è¯¯ï¼‰
        try:
            if hasattr(self, '_generate_detailed_comparison'):
                self._generate_detailed_comparison(results, dataset_name, downstream_model)
            if hasattr(self, '_generate_comparison_report'):
                self._generate_comparison_report(results, dataset_name, downstream_model)
            if hasattr(self, '_plot_comparison'):
                self._plot_comparison(results, dataset_name, downstream_model)
        except Exception as e:
            print(f"âš ï¸ å¯é€‰çš„è¯¦ç»†æŠ¥å‘Šç”Ÿæˆè·³è¿‡: {e}")

        print(f"âœ… ç»¼åˆç»“æœè®¡ç®—å®Œæˆï¼Œä¿å­˜åˆ°: {comprehensive_dir}")

        return {
            'experiment_dir': str(exp_dir),
            'models_processed': list(available_models),
            'comprehensive_results_dir': str(comprehensive_dir),
            'results': results
        }

    def batch_compute_comprehensive_results(self,
                                          results_base_dir: str = None,
                                          experiment_pattern: str = "*",
                                          model_names: List[str] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡è®¡ç®—å¤šä¸ªå®éªŒçš„ç»¼åˆç»“æœ

        Args:
            results_base_dir: ç»“æœåŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨self.results_dir
            experiment_pattern: å®éªŒç›®å½•åŒ¹é…æ¨¡å¼ï¼ˆå¦‚ "Cora_*_gcn"ï¼‰
            model_names: è¦åŒ…å«çš„æ¨¡å‹åç§°åˆ—è¡¨

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœå­—å…¸
        """
        if results_base_dir is None:
            results_base_dir = self.results_dir
        else:
            results_base_dir = Path(results_base_dir)

        print(f"ğŸ”„ æ‰¹é‡è®¡ç®—ç»¼åˆç»“æœ: {results_base_dir} / {experiment_pattern}")

        # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„å®éªŒç›®å½•
        experiment_dirs = list(results_base_dir.glob(experiment_pattern))
        experiment_dirs = [d for d in experiment_dirs if d.is_dir()]

        if not experiment_dirs:
            raise ValueError(f"æœªæ‰¾åˆ°åŒ¹é…çš„å®éªŒç›®å½•: {results_base_dir} / {experiment_pattern}")

        print(f"ğŸ“‚ æ‰¾åˆ° {len(experiment_dirs)} ä¸ªå®éªŒç›®å½•")

        batch_results = {}
        successful_count = 0

        for exp_dir in experiment_dirs:
            exp_name = exp_dir.name
            print(f"\nğŸ”¬ å¤„ç†å®éªŒ: {exp_name}")

            try:
                result = self.compute_comprehensive_results_from_process_data(
                    experiment_dir=str(exp_dir),
                    model_names=model_names
                )
                batch_results[exp_name] = result
                successful_count += 1

            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                batch_results[exp_name] = {'error': str(e)}

        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ: {successful_count}/{len(experiment_dirs)} ä¸ªå®éªŒæˆåŠŸ")

        return {
            'processed_experiments': batch_results,
            'success_count': successful_count,
            'total_count': len(experiment_dirs)
        }

    def _log_memory_usage(self, stage: str = ""):
        """Log current memory usage"""
        if not self.memory_monitor:
            return

        # System memory
        system_memory = psutil.virtual_memory()
        process = psutil.Process(os.getpid())
        process_memory = process.memory_info()

        print(f"ğŸ” Memory usage{f' ({stage})' if stage else ''}:")
        print(f"  System: {system_memory.used / 1024**3:.2f}GB / {system_memory.total / 1024**3:.2f}GB "
              f"({system_memory.percent:.1f}%)")
        print(f"  Process: {process_memory.rss / 1024**3:.2f}GB")

        # CUDA memory if available
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  CUDA {i}: {allocated:.2f}GB allocated, {cached:.2f}GB cached / {total:.2f}GB total")

    def _cleanup_memory(self):
        """Perform comprehensive memory cleanup"""
        # Collect garbage
        gc.collect()

        # Clear CUDA cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            # Force synchronization
            torch.cuda.synchronize()

        print("ğŸ§¹ Memory cleanup completed")

    def _get_adaptive_epochs(self, requested_epochs: int, num_steps: int) -> int:
        """Calculate adaptive epochs based on available memory and number of steps"""
        if not self.enable_memory_optimization:
            return requested_epochs

        # Basic heuristic: reduce epochs for larger step counts
        if num_steps > 15:
            return max(requested_epochs // 3, 20)
        elif num_steps > 10:
            return max(requested_epochs // 2, 30)
        else:
            return requested_epochs

    def _estimate_memory_requirements(self, graph: Data, num_steps: int) -> float:
        """Estimate memory requirements in GB for a given graph and step count"""
        num_nodes = graph.num_nodes
        num_edges = graph.edge_index.shape[1]
        feature_dim = graph.x.shape[1]

        # Rough estimation based on graph size
        graph_memory_mb = (num_nodes * feature_dim * 4 + num_edges * 2 * 4) / 1024**2  # MB
        model_memory_mb = 50  # Typical downstream model size in MB
        total_memory_gb = (graph_memory_mb + model_memory_mb * num_steps) / 1024

        return total_memory_gb

    def _check_memory_feasibility(self, graph: Data, num_steps: int) -> Tuple[bool, str]:
        """Check if the computation is feasible with current memory"""
        estimated_memory = self._estimate_memory_requirements(graph, num_steps)

        if torch.cuda.is_available():
            device_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            free_memory = device_memory - torch.cuda.memory_allocated(0) / 1024**3

            if estimated_memory > free_memory * 0.8:  # Use 80% threshold
                return False, f"Estimated memory ({estimated_memory:.2f}GB) exceeds available ({free_memory:.2f}GB)"

        return True, "Memory check passed"

    def run_ppi_multi_label_benchmark(self,
                                     model_names: List[str],
                                     label_indices: List[int] = None,
                                     downstream_model: str = 'gcn',
                                     num_steps: int = 5,
                                     epochs: int = 30) -> Dict[str, Any]:
        """
        Run benchmark on PPI dataset for multiple binary classification tasks

        Args:
            model_names: List of model names to test
            label_indices: List of label indices to test (0-120). If None, test first 10 labels
            downstream_model: Downstream model type
            num_steps: Number of summarization steps
            epochs: Training epochs

        Returns:
            Dictionary containing results for all label tasks
        """
        if label_indices is None:
            # Test first 10 labels by default to avoid excessive computation
            label_indices = list(range(10))

        print(f"\n{'='*120}")
        print(f"PPI Multi-Label Benchmark")
        print(f"æ¨¡å‹: {model_names}")
        print(f"æ ‡ç­¾ä»»åŠ¡: {len(label_indices)} ä¸ª (indices: {label_indices})")
        print(f"{'='*120}")

        all_results = {}
        summary_data = []

        total_experiments = len(model_names) * len(label_indices)
        current_experiment = 0

        for label_idx in label_indices:
            task_key = f"PPI_label_{label_idx}"
            print(f"\n{'='*80}")
            print(f"PPI æ ‡ç­¾ä»»åŠ¡ {label_idx}")
            print(f"{'='*80}")

            task_results = []
            for model_name in model_names:
                current_experiment += 1
                print(f"\n[{current_experiment}/{total_experiments}] æµ‹è¯• {model_name} on label {label_idx}...")

                try:
                    # Include PPI label index in model kwargs
                    model_kwargs = {'ppi_label_index': label_idx}

                    result = self.run_single_model(
                        model_name=model_name,
                        dataset_name='PPI',
                        task_type='original',
                        downstream_model=downstream_model,
                        num_steps=num_steps,
                        epochs=epochs,
                        model_kwargs=model_kwargs
                    )

                    if result.get('success', False):
                        task_results.append(result)
                        summary_data.append({
                            'label_index': label_idx,
                            'model': model_name,
                            'category': result['model_info']['category'],
                            'snr_auc': result['snr_auc'],
                            'training_time': result.get('training_time', 0),
                            'summarization_time': result.get('summarization_time', 0)
                        })
                        print(f"âœ… {model_name} on label {label_idx}: SNR-AUC = {result['snr_auc']:.4f}")
                    else:
                        print(f"âŒ {model_name} on label {label_idx} æµ‹è¯•å¤±è´¥")

                except Exception as e:
                    print(f"âŒ {model_name} on label {label_idx} æµ‹è¯•å‡ºé”™: {e}")

            # Save results for this label task
            if task_results:
                all_results[task_key] = {
                    'results': task_results,
                    'label_index': label_idx,
                    'dataset': 'PPI'
                }

        # Generate comprehensive report
        self._generate_ppi_multi_label_report(summary_data, downstream_model)

        return {
            'all_results': all_results,
            'summary': summary_data,
            'total_experiments': total_experiments,
            'success_rate': len(summary_data) / total_experiments if total_experiments > 0 else 0,
            'tested_labels': label_indices
        }

    def _generate_ppi_multi_label_report(self, summary_data: List[Dict], downstream_model: str):
        """
        Generate comprehensive report for PPI multi-label benchmark
        """
        if not summary_data:
            print("æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return

        df = pd.DataFrame(summary_data)

        # Save detailed results
        detailed_path = self.results_dir / f'ppi_multi_label_detailed_{downstream_model}.csv'
        df.to_csv(detailed_path, index=False)
        print(f"\nğŸ“Š PPIå¤šæ ‡ç­¾è¯¦ç»†ç»“æœä¿å­˜åˆ°: {detailed_path}")

        # Generate summary table by model
        model_summary = df.groupby(['model', 'category']).agg({
            'snr_auc': ['mean', 'std', 'min', 'max', 'count'],
            'training_time': 'mean',
            'summarization_time': 'mean'
        }).round(4)

        summary_path = self.results_dir / f'ppi_multi_label_summary_{downstream_model}.tsv'
        model_summary.to_csv(summary_path, sep='\t')
        print(f"ğŸ“Š PPIå¤šæ ‡ç­¾æ±‡æ€»è¡¨ä¿å­˜åˆ°: {summary_path}")

        # Print summary statistics
        print(f"\n{'='*120}")
        print(f"PPIå¤šæ ‡ç­¾åŸºå‡†æµ‹è¯•æ±‡æ€» ({downstream_model.upper()})")
        print(f"{'='*120}")

        print(f"æµ‹è¯•çš„æ ‡ç­¾æ•°é‡: {df['label_index'].nunique()}")
        print(f"æµ‹è¯•çš„æ¨¡å‹æ•°é‡: {df['model'].nunique()}")
        print(f"æ€»å®éªŒæ¬¡æ•°: {len(df)}")

        print(f"\næ¨¡å‹å¹³å‡æ€§èƒ½ (è·¨æ‰€æœ‰æ ‡ç­¾):")
        for (model, category), group in df.groupby(['model', 'category']):
            mean_auc = group['snr_auc'].mean()
            std_auc = group['snr_auc'].std()
            print(f"  {model} ({category}): {mean_auc:.4f} Â± {std_auc:.4f}")

        # Best performing label tasks
        best_by_label = df.loc[df.groupby('label_index')['snr_auc'].idxmax()]
        print(f"\nå„æ ‡ç­¾ä»»åŠ¡æœ€ä½³æ¨¡å‹:")
        for _, row in best_by_label.iterrows():
            print(f"  Label {row['label_index']}: {row['model']} (SNR-AUC: {row['snr_auc']:.4f})")